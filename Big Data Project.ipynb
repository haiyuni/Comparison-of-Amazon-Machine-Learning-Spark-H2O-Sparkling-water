{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Loading and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the data as a CSV. Note I also showed a row for simplicity to understand what we're looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    ".option(\"header\",True) \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".load('file:/home/cloudera/Big Data Project/final_with_churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20170306</td>\n",
       "      <td>20151107</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  ++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=          0   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                  16           \\N           \\N           \\N            \\N   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs  avg_plan_length  \\\n",
       "0            \\N               \\N             \\N             30.0   \n",
       "\n",
       "   avg_expected_plan_price  avg_actual_plan_price  max_expiration_date  \\\n",
       "0                     99.0                   99.0             20170306   \n",
       "\n",
       "   min_transaction_date city age gender registered_via  is_churn  \n",
       "0              20151107   \\N  \\N     \\N             \\N         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to check the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- days_used: integer (nullable = true)\n",
      " |-- total_transactions: integer (nullable = true)\n",
      " |-- total_num_25: string (nullable = true)\n",
      " |-- total_num_50: string (nullable = true)\n",
      " |-- total_num_75: string (nullable = true)\n",
      " |-- total_num_985: string (nullable = true)\n",
      " |-- total_num_100: string (nullable = true)\n",
      " |-- avg_unique_songs: string (nullable = true)\n",
      " |-- avg_total_secs: string (nullable = true)\n",
      " |-- avg_plan_length: double (nullable = true)\n",
      " |-- avg_expected_plan_price: double (nullable = true)\n",
      " |-- avg_actual_plan_price: double (nullable = true)\n",
      " |-- max_expiration_date: integer (nullable = true)\n",
      " |-- min_transaction_date: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- registered_via: string (nullable = true)\n",
      " |-- is_churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that sum of the numbers came in as strings due to NA values. We're going to have to clean next so there are 2 steps to this:\n",
    "\n",
    "1. Filling \\N's in the appropriate columsns with 0. We will also want to convert the strings in these fields to floats.\n",
    "    - these columns are\n",
    "2. Dropping rows that aren't relevant because they have missing data.\n",
    "    - these columns are: city, age, gender, registered via\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get a row count so we have some idea of proportionality moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992931"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select('user_id').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting 1, lets check how many are missing from these columns. Note that I assume if one column is missing they are all misssing as they all were pulled from the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|total_num_25| count|\n",
      "+------------+------+\n",
      "|          \\N|123005|\n",
      "|           0|  2684|\n",
      "|          36|   845|\n",
      "|          72|   776|\n",
      "|          60|   751|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('total_num_25').groupby('total_num_25').count().sort('count', ascending = False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 123k missing the values pulled from the usage. That being said, since these users were present in other tables I'm going to make the assumption they were registered but for whatever reason they did not use their account actively. In this case, we will leave them in, but fill these usage related values as 0's in our next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1\n",
    "data = data.withColumn('total_num_25', \\\n",
    "                      when(data['total_num_25']=='\\N', '0.0').otherwise(data['total_num_25']))\n",
    "\n",
    "data = data.withColumn('total_num_50', \\\n",
    "                      when(data['total_num_50']=='\\N', '0.0').otherwise(data['total_num_50']))\n",
    "\n",
    "data = data.withColumn('total_num_75', \\\n",
    "                      when(data['total_num_75']=='\\N', '0.0').otherwise(data['total_num_75']))\n",
    "\n",
    "data = data.withColumn('total_num_985', \\\n",
    "                      when(data['total_num_985']=='\\N', '0.0').otherwise(data['total_num_985']))\n",
    "\n",
    "data = data.withColumn('total_num_100', \\\n",
    "                      when(data['total_num_100']=='\\N', '0.0').otherwise(data['total_num_100']))\n",
    "\n",
    "data = data.withColumn('avg_unique_songs', \\\n",
    "                      when(data['avg_unique_songs']=='\\N', '0.0').otherwise(data['avg_unique_songs']))\n",
    "\n",
    "data = data.withColumn('avg_total_secs', \\\n",
    "                      when(data['avg_total_secs']=='\\N', '0.0').otherwise(data['avg_total_secs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the first row to make sure the above corrections worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20170306</td>\n",
       "      <td>20151107</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  ++0wqjjQge1mBBe5r4ciHGKwtF/m322zkra7CK8I+Mw=          0   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                  16          0.0          0.0          0.0           0.0   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs  avg_plan_length  \\\n",
       "0           0.0              0.0            0.0             30.0   \n",
       "\n",
       "   avg_expected_plan_price  avg_actual_plan_price  max_expiration_date  \\\n",
       "0                     99.0                   99.0             20170306   \n",
       "\n",
       "   min_transaction_date city age gender registered_via  is_churn  \n",
       "0              20151107   \\N  \\N     \\N             \\N         0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will try to drop rows missing some values... (city, age, gender, registered via) as these are important attributes we want in our analysis. First lets see how many rows are missing these values; note taht i assumed if its missing for city its missing for gender/age/registered via as well since they all pulled from the same table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age| count|\n",
      "+---+------+\n",
      "|  0|433567|\n",
      "| \\N|296297|\n",
      "| 27| 15344|\n",
      "| 26| 14611|\n",
      "| 25| 13778|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('age').groupby('age').count().sort('count', ascending = False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "296,297 is a little more than a quarter of rows misssing these values; its a substantial number but since these attributes were from their customer table, I find it a bit odd that the customer table is missing this data while the payment table still has it. From earlier analysis, I could also see a lot of these rows were also missing usage data. Since we are testing the capabilities of pyspark, I am going to make the executive decision to drop these rows; we will still have a viable model built off a large quantity of data without them and will be able to drive better insights with the data not being so present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.filter(data.age != '\\N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking first row to make sure it worked... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++Bks8kE9oclzxZM3hcWs+qzsxuoXFeIE1+7pxKBCQg=</td>\n",
       "      <td>624</td>\n",
       "      <td>624</td>\n",
       "      <td>2408</td>\n",
       "      <td>312</td>\n",
       "      <td>264</td>\n",
       "      <td>360</td>\n",
       "      <td>18864</td>\n",
       "      <td>20.76923</td>\n",
       "      <td>6836.1167</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>20170331</td>\n",
       "      <td>20160730</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  ++Bks8kE9oclzxZM3hcWs+qzsxuoXFeIE1+7pxKBCQg=        624   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                 624         2408          312          264           360   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs  avg_plan_length  \\\n",
       "0         18864         20.76923      6836.1167             30.0   \n",
       "\n",
       "   avg_expected_plan_price  avg_actual_plan_price  max_expiration_date  \\\n",
       "0                     99.0                   99.0             20170331   \n",
       "\n",
       "   min_transaction_date city age gender registered_via  is_churn  \n",
       "0              20160730    1   0                     7         0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking counts again to make sure all NA are gone...\n",
    "\n",
    "Note that some are still null for female/male but since they were included in the rest the table we will leave that in. Perhaps the empty value for this field is because including gender is optional, and maybe our model will take that input as some type of value (People who don't put down there gender are more likely to churn???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|      gender| count|\n",
      "+------------+------+\n",
      "|Not_Provided|430471|\n",
      "|        male|141045|\n",
      "|      female|125118|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('gender',when(\n",
    "data['gender']=='', 'Not_Provided').otherwise(data['gender']))\n",
    "\n",
    "data.select('gender').groupby('gender').count().sort('count', ascending = False).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final thing we have to do is find the date difference between the max_expiration_date and the min_transaction_date.\n",
    "\n",
    "First we need to convert both to dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- days_used: integer (nullable = true)\n",
      " |-- total_transactions: integer (nullable = true)\n",
      " |-- total_num_25: string (nullable = true)\n",
      " |-- total_num_50: string (nullable = true)\n",
      " |-- total_num_75: string (nullable = true)\n",
      " |-- total_num_985: string (nullable = true)\n",
      " |-- total_num_100: string (nullable = true)\n",
      " |-- avg_unique_songs: string (nullable = true)\n",
      " |-- avg_total_secs: string (nullable = true)\n",
      " |-- avg_plan_length: double (nullable = true)\n",
      " |-- avg_expected_plan_price: double (nullable = true)\n",
      " |-- avg_actual_plan_price: double (nullable = true)\n",
      " |-- max_expiration_date: string (nullable = true)\n",
      " |-- min_transaction_date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- registered_via: string (nullable = true)\n",
      " |-- is_churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('max_expiration_date',data.max_expiration_date.cast(\"string\"))\n",
    "data = data.withColumn('min_transaction_date',data.min_transaction_date.cast(\"string\"))\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|max_expiration_date|min_transaction_date|\n",
      "+-------------------+--------------------+\n",
      "|           20170331|            20160730|\n",
      "|           20170311|            20150914|\n",
      "|           20170316|            20150131|\n",
      "|           20170331|            20150121|\n",
      "|           20170305|            20150410|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable('data')\n",
    "\n",
    "data.select('max_expiration_date','min_transaction_date').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#building the correct date vectors\n",
    "max_exp_date = sqlContext.sql(\"\"\"\n",
    "    SELECT TO_DATE(CAST(UNIX_TIMESTAMP(max_expiration_date,'yyyyMMdd') AS TIMESTAMP)) AS max_exp_date\n",
    "    FROM DATA\n",
    "\"\"\")\n",
    "\n",
    "min_trans_date = sqlContext.sql(\"\"\"\n",
    "    SELECT TO_DATE(CAST(UNIX_TIMESTAMP(min_transaction_date,'yyyyMMdd') AS TIMESTAMP)) AS min_trans_date\n",
    "    FROM DATA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|max_exp_date|\n",
      "+------------+\n",
      "|  2017-03-31|\n",
      "|  2017-03-11|\n",
      "|  2017-03-16|\n",
      "|  2017-03-31|\n",
      "|  2017-03-05|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_exp_date.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|min_trans_date|\n",
      "+--------------+\n",
      "|    2016-07-30|\n",
      "|    2015-09-14|\n",
      "|    2015-01-31|\n",
      "|    2015-01-21|\n",
      "|    2015-04-10|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_trans_date.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merging these into the data dataFrame\n",
    "\n",
    "data = data.withColumn('row_index', monotonically_increasing_id())\n",
    "max_exp_date = max_exp_date.withColumn('row_index', monotonically_increasing_id())\n",
    "min_trans_date = min_trans_date.withColumn('row_index', monotonically_increasing_id())\n",
    "\n",
    "data = data.join(max_exp_date, on=['row_index'])\n",
    "data = data.join(min_trans_date, on=['row_index']).drop('row_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make date diff column\n",
    "\n",
    "#for some reason tihs is all 0s.... fix somehow. the min trans date is coming in wrong\n",
    "data = data.withColumn('date_diff', datediff(data.max_exp_date,data.min_trans_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>...</th>\n",
       "      <th>max_expiration_date</th>\n",
       "      <th>min_transaction_date</th>\n",
       "      <th>city</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "      <th>max_exp_date</th>\n",
       "      <th>min_trans_date</th>\n",
       "      <th>date_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+1ZgRw2ZlmD4Z1NCVo8lh4ECpNtG73bp/cECvhq4l8Q=</td>\n",
       "      <td>1071</td>\n",
       "      <td>1071</td>\n",
       "      <td>2961</td>\n",
       "      <td>1050</td>\n",
       "      <td>735</td>\n",
       "      <td>651</td>\n",
       "      <td>16212</td>\n",
       "      <td>16.745098</td>\n",
       "      <td>4050.471</td>\n",
       "      <td>...</td>\n",
       "      <td>20170311</td>\n",
       "      <td>20150802</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-11</td>\n",
       "      <td>2015-08-02</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+NKVPkGwpoOWKQDdH3mtpaZGR5lx9fu5bOHixIRjsnI=</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>17385</td>\n",
       "      <td>3495</td>\n",
       "      <td>3270</td>\n",
       "      <td>2580</td>\n",
       "      <td>75045</td>\n",
       "      <td>18.718391</td>\n",
       "      <td>8045.0156</td>\n",
       "      <td>...</td>\n",
       "      <td>20170315</td>\n",
       "      <td>20151216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+i2T+lq7TNR/gThVOEh6M3CdHEIgZhgeH1ENTjAgMyE=</td>\n",
       "      <td>6403</td>\n",
       "      <td>6403</td>\n",
       "      <td>39349</td>\n",
       "      <td>10260</td>\n",
       "      <td>6175</td>\n",
       "      <td>5966</td>\n",
       "      <td>79591</td>\n",
       "      <td>19.973293</td>\n",
       "      <td>3643.7224</td>\n",
       "      <td>...</td>\n",
       "      <td>20170319</td>\n",
       "      <td>20150920</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-19</td>\n",
       "      <td>2015-09-20</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/67f8zgh70yyzqwntxaAuqqSrbibNC7KxG5rGBg4/hc=</td>\n",
       "      <td>130</td>\n",
       "      <td>130</td>\n",
       "      <td>390</td>\n",
       "      <td>55</td>\n",
       "      <td>80</td>\n",
       "      <td>65</td>\n",
       "      <td>1970</td>\n",
       "      <td>12.230769</td>\n",
       "      <td>4521.8457</td>\n",
       "      <td>...</td>\n",
       "      <td>20170331</td>\n",
       "      <td>20161029</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>2016-10-29</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/UYp4Ued/yMVEf5OD13C9Hz8B/N78PBx13tglE3+gXA=</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>213</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>2409</td>\n",
       "      <td>19.564102</td>\n",
       "      <td>5146.8057</td>\n",
       "      <td>...</td>\n",
       "      <td>20170331</td>\n",
       "      <td>20161231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        user_id  days_used  \\\n",
       "0  +1ZgRw2ZlmD4Z1NCVo8lh4ECpNtG73bp/cECvhq4l8Q=       1071   \n",
       "1  +NKVPkGwpoOWKQDdH3mtpaZGR5lx9fu5bOHixIRjsnI=       2610   \n",
       "2  +i2T+lq7TNR/gThVOEh6M3CdHEIgZhgeH1ENTjAgMyE=       6403   \n",
       "3  /67f8zgh70yyzqwntxaAuqqSrbibNC7KxG5rGBg4/hc=        130   \n",
       "4  /UYp4Ued/yMVEf5OD13C9Hz8B/N78PBx13tglE3+gXA=        117   \n",
       "\n",
       "   total_transactions total_num_25 total_num_50 total_num_75 total_num_985  \\\n",
       "0                1071         2961         1050          735           651   \n",
       "1                2610        17385         3495         3270          2580   \n",
       "2                6403        39349        10260         6175          5966   \n",
       "3                 130          390           55           80            65   \n",
       "4                 117          213           21            9            15   \n",
       "\n",
       "  total_num_100 avg_unique_songs avg_total_secs    ...      \\\n",
       "0         16212        16.745098       4050.471    ...       \n",
       "1         75045        18.718391      8045.0156    ...       \n",
       "2         79591        19.973293      3643.7224    ...       \n",
       "3          1970        12.230769      4521.8457    ...       \n",
       "4          2409        19.564102      5146.8057    ...       \n",
       "\n",
       "   max_expiration_date  min_transaction_date  city age        gender  \\\n",
       "0             20170311              20150802     1   0  Not_Provided   \n",
       "1             20170315              20151216     1   0  Not_Provided   \n",
       "2             20170319              20150920     1   0  Not_Provided   \n",
       "3             20170331              20161029     1   0  Not_Provided   \n",
       "4             20170331              20161231     1   0  Not_Provided   \n",
       "\n",
       "  registered_via is_churn max_exp_date min_trans_date  date_diff  \n",
       "0              7        0   2017-03-11     2015-08-02        587  \n",
       "1              7        0   2017-03-15     2015-12-16        455  \n",
       "2              7        0   2017-03-19     2015-09-20        546  \n",
       "3              7        0   2017-03-31     2016-10-29        153  \n",
       "4              7        0   2017-03-31     2016-12-31         90  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prep and Pipeline Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we need to get our data in a format that can be fed to models. First, we will select the data and also cast it to its appropriate data types now that its been cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- days_used: integer (nullable = true)\n",
      " |-- total_transactions: integer (nullable = true)\n",
      " |-- total_num_25: string (nullable = true)\n",
      " |-- total_num_50: string (nullable = true)\n",
      " |-- total_num_75: string (nullable = true)\n",
      " |-- total_num_985: string (nullable = true)\n",
      " |-- total_num_100: string (nullable = true)\n",
      " |-- avg_unique_songs: string (nullable = true)\n",
      " |-- avg_total_secs: string (nullable = true)\n",
      " |-- avg_plan_length: double (nullable = true)\n",
      " |-- avg_expected_plan_price: double (nullable = true)\n",
      " |-- avg_actual_plan_price: double (nullable = true)\n",
      " |-- max_expiration_date: string (nullable = true)\n",
      " |-- min_transaction_date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- registered_via: string (nullable = true)\n",
      " |-- is_churn: integer (nullable = true)\n",
      " |-- max_exp_date: date (nullable = true)\n",
      " |-- min_trans_date: date (nullable = true)\n",
      " |-- date_diff: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_used</th>\n",
       "      <th>total_transactions</th>\n",
       "      <th>total_num_25</th>\n",
       "      <th>total_num_50</th>\n",
       "      <th>total_num_75</th>\n",
       "      <th>total_num_985</th>\n",
       "      <th>total_num_100</th>\n",
       "      <th>avg_unique_songs</th>\n",
       "      <th>avg_total_secs</th>\n",
       "      <th>avg_plan_length</th>\n",
       "      <th>avg_expected_plan_price</th>\n",
       "      <th>avg_actual_plan_price</th>\n",
       "      <th>date_diff</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1071.0</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>2961.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>16212.0</td>\n",
       "      <td>16.745098</td>\n",
       "      <td>4050.471</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.04762</td>\n",
       "      <td>99.04762</td>\n",
       "      <td>587.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Not_Provided</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   days_used  total_transactions  total_num_25  total_num_50  total_num_75  \\\n",
       "0     1071.0              1071.0        2961.0        1050.0         735.0   \n",
       "\n",
       "   total_num_985  total_num_100  avg_unique_songs  avg_total_secs  \\\n",
       "0          651.0        16212.0         16.745098        4050.471   \n",
       "\n",
       "   avg_plan_length  avg_expected_plan_price  avg_actual_plan_price  date_diff  \\\n",
       "0             30.0                 99.04762               99.04762      587.0   \n",
       "\n",
       "   age city        gender registered_via  is_churn  \n",
       "0  0.0    1  Not_Provided              7       0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = data.select(data.days_used.cast('double'),\n",
    "                   data.total_transactions.cast('double'),\n",
    "                   data.total_num_25.cast('double'),\n",
    "                   data.total_num_50.cast('double'),\n",
    "                   data.total_num_75.cast('double'),\n",
    "                   data.total_num_985.cast('double'),\n",
    "                   data.total_num_100.cast('double'),\n",
    "                   data.avg_unique_songs.cast('double'),\n",
    "                   data.avg_total_secs.cast('double'),\n",
    "                   data.avg_plan_length.cast('double'),\n",
    "                   data.avg_expected_plan_price.cast('double'),\n",
    "                   data.avg_actual_plan_price.cast('double'),\n",
    "                   data.date_diff.cast('double'),\n",
    "                   data.age.cast('double'),\n",
    "                   data.city,\n",
    "                   data.gender,\n",
    "                   data.registered_via,\n",
    "                   data.is_churn.cast('double')               \n",
    "                  )\n",
    "\n",
    "cols.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing categorical variables, then encoding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer)\n",
    "\n",
    "#City, gender, registered via\n",
    "#index\n",
    "city_indexer = StringIndexer(inputCol='city', outputCol='cityIndex')\n",
    "gender_indexer = StringIndexer(inputCol='gender', outputCol='genderIndex')\n",
    "registered_via_indexer = StringIndexer(inputCol='registered_via', outputCol='registered_viaIndex')\n",
    "\n",
    "#encoders\n",
    "city_encoder = OneHotEncoder(inputCol='cityIndex', outputCol='cityVec')\n",
    "gender_encoder = OneHotEncoder(inputCol='genderIndex', outputCol='genderVec')\n",
    "registered_via_encoder = OneHotEncoder(inputCol='registered_viaIndex', outputCol='registered_viaVec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['days_used','total_transactions','total_num_25','total_num_50','total_num_75',\n",
    "                                      'total_num_985','total_num_100','avg_unique_songs','avg_total_secs','avg_plan_length',\n",
    "                                      'avg_expected_plan_price','avg_actual_plan_price','date_diff','age','cityVec','genderVec',\n",
    "                                      'registered_viaVec'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and setting up logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(labelCol='is_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe = Pipeline(stages=[city_indexer,\n",
    "                       gender_indexer,\n",
    "                       registered_via_indexer,\n",
    "                       city_encoder,\n",
    "                       gender_encoder,\n",
    "                       registered_via_encoder,\n",
    "                       assembler,\n",
    "                       log_reg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data to a 70 / 30 - test / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = cols.randomSplit([.7,.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit = pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "results = fit.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg_model = fit.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.76861738294e-06,-2.76906642165e-06,2.76129596462e-08,1.10384579839e-07,-1.41549541285e-07,-1.76662626343e-07,-3.33158865124e-08,0.00119160927337,3.12806197438e-17,0.00183171592377,0.000966000200896,0.000856394610835,-0.000108746679117,0.00070305104041,-0.073810076421,0.0325104039908,0.0429620802755,0.041198160646,0.060604062683,0.0571704979203,0.0841774793329,0.0115678181962,0.063762188967,0.0369474132174,0.0575373418642,0.0211504365713,0.106722215846,0.0572012113462,0.0205644179312,0.0748219495581,0.0547873462334,0.011295756786,0.0172637063013,-0.0294404827105,-0.0802804552893,0.063089695364,-0.191192325923,0.0736578550748,0.215443133875,0.325510511337]\n"
     ]
    }
   ],
   "source": [
    "print log_reg_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835591994753\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "data_eval = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='is_churn')\n",
    "\n",
    "auc = data_eval.evaluate(results)\n",
    "\n",
    "print auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build confusion matrix\n",
    "predicted = results.map(lambda b: b[3])\n",
    "\n",
    "a.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelineModel' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-717852de017d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelineModel' object has no attribute 'predict'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dec_tree = DecisionTreeClassifier(labelCol='is_churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc_pipe = Pipeline(stages=[city_indexer,\n",
    "                       gender_indexer,\n",
    "                       registered_via_indexer,\n",
    "                       city_encoder,\n",
    "                       gender_encoder,\n",
    "                       registered_via_encoder,\n",
    "                       assembler,\n",
    "                       dec_tree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'DecisionTreeClassifier was given input with invalid label column is_churn, without the number of classes specified. See StringIndexer.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-7c85e2b7d63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdc_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'DecisionTreeClassifier was given input with invalid label column is_churn, without the number of classes specified. See StringIndexer.'"
     ]
    }
   ],
   "source": [
    "dc_fit = dc_pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc_results = dc_fit.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc_data_eval = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='is_churn')\n",
    "\n",
    "dc_auc = dc_data_eval.evaluate(dc_results)\n",
    "\n",
    "print dc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
